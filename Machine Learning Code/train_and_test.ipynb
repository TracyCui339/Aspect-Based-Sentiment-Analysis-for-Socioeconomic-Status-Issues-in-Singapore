{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from torch) (1.24.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.0.0rc1\n",
      "  Using cached transformers-4.0.0rc1-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers==4.0.0rc1) (0.9.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers==4.0.0rc1) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers==4.0.0rc1) (4.64.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers==4.0.0rc1) (0.0.53)\n",
      "Requirement already satisfied: requests in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers==4.0.0rc1) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers==4.0.0rc1) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers==4.0.0rc1) (2022.7.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers==4.0.0rc1) (1.24.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.0.0rc1) (0.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from packaging->transformers==4.0.0rc1) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from requests->transformers==4.0.0rc1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from requests->transformers==4.0.0rc1) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from requests->transformers==4.0.0rc1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from requests->transformers==4.0.0rc1) (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.0.0rc1) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.0.0rc1) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.0.0rc1) (8.0.4)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.0.0rc1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[gpu] in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Collecting transformers[gpu]\n",
      "  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers[gpu]) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers[gpu]) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers[gpu]) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers[gpu]) (1.24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers[gpu]) (2022.7.9)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers[gpu]) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers[gpu]) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from transformers[gpu]) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[gpu]) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers[gpu]) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[gpu]) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from requests->transformers[gpu]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from requests->transformers[gpu]) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from requests->transformers[gpu]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tracycui\\anaconda3\\lib\\site-packages (from requests->transformers[gpu]) (2022.12.7)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.4\n",
      "    Uninstalling tokenizers-0.9.4:\n",
      "      Successfully uninstalled tokenizers-0.9.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.0.0rc1\n",
      "    Uninstalling transformers-4.0.0rc1:\n",
      "      Successfully uninstalled transformers-4.0.0rc1\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.28.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: transformers 4.28.1 does not provide the extra 'gpu'\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "j52fZ12P78E3"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# file: train.py\n",
    "# author: songyouwei <youwei0314@gmail.com>\n",
    "# Copyright (C) 2018. All Rights Reserved.\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "from sklearn import metrics\n",
    "from time import strftime, localtime\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from data_utils import build_tokenizer, build_embedding_matrix, Tokenizer4Bert, ABSADataset\n",
    "from models import LSTM, IAN, MemNet, RAM, TD_LSTM, TC_LSTM, Cabasc, ATAE_LSTM, TNet_LF, AOA, MGAN, ASGCN, LCF_BERT\n",
    "from models.aen import CrossEntropyLoss_LSR, AEN_BERT\n",
    "from models.bert_spc import BERT_SPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "e4zNFBBU78E4"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "wuxhcWP978E4"
   },
   "outputs": [],
   "source": [
    "class Instructor:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "\n",
    "        if 'bert' in opt.model_name:\n",
    "            tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.pretrained_bert_name)\n",
    "            bert = BertModel.from_pretrained(opt.pretrained_bert_name, return_dict=False)\n",
    "            self.model = opt.model_class(bert, opt).to(opt.device)\n",
    "        else:\n",
    "            tokenizer = build_tokenizer(\n",
    "                fnames=[opt.dataset_file['train'], opt.dataset_file['test']],\n",
    "                max_seq_len=opt.max_seq_len,\n",
    "                dat_fname='{0}_tokenizer.dat'.format(opt.dataset))\n",
    "            embedding_matrix = build_embedding_matrix(\n",
    "                word2idx=tokenizer.word2idx,\n",
    "                embed_dim=opt.embed_dim,\n",
    "                dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(opt.embed_dim), opt.dataset))\n",
    "            self.model = opt.model_class(embedding_matrix, opt).to(opt.device)\n",
    "\n",
    "        self.trainset = ABSADataset(opt.dataset_file['train'], tokenizer)\n",
    "        self.testset = ABSADataset(opt.dataset_file['test'], tokenizer)\n",
    "        assert 0 <= opt.valset_ratio < 1\n",
    "        if opt.valset_ratio > 0:\n",
    "            valset_len = int(len(self.trainset) * opt.valset_ratio)\n",
    "            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
    "        else:\n",
    "            self.valset = self.testset\n",
    "\n",
    "        if opt.device.type == 'cuda':\n",
    "            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))\n",
    "        self._print_args()\n",
    "\n",
    "    def _print_args(self):\n",
    "        n_trainable_params, n_nontrainable_params = 0, 0\n",
    "        for p in self.model.parameters():\n",
    "            n_params = torch.prod(torch.tensor(p.shape))\n",
    "            if p.requires_grad:\n",
    "                n_trainable_params += n_params\n",
    "            else:\n",
    "                n_nontrainable_params += n_params\n",
    "        logger.info('> n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
    "        logger.info('> training arguments:')\n",
    "        for arg in vars(self.opt):\n",
    "            logger.info('>>> {0}: {1}'.format(arg, getattr(self.opt, arg)))\n",
    "\n",
    "    def _reset_params(self):\n",
    "        for child in self.model.children():\n",
    "            if type(child) != BertModel:  # skip bert params\n",
    "                for p in child.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        if len(p.shape) > 1:\n",
    "                            self.opt.initializer(p)\n",
    "                        else:\n",
    "                            stdv = 1. / math.sqrt(p.shape[0])\n",
    "                            torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "\n",
    "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
    "        max_val_acc = 0\n",
    "        max_val_f1 = 0\n",
    "        max_val_epoch = 0\n",
    "        global_step = 0\n",
    "        path = None\n",
    "        for i_epoch in range(self.opt.num_epoch):\n",
    "            logger.info('>' * 100)\n",
    "            logger.info('epoch: {}'.format(i_epoch))\n",
    "            n_correct, n_total, loss_total = 0, 0, 0\n",
    "            # switch model to training mode\n",
    "            self.model.train()\n",
    "            for i_batch, batch in enumerate(train_data_loader):\n",
    "                global_step += 1\n",
    "                # clear gradient accumulators\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "\n",
    "                inputs = [batch[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                outputs = self.model(inputs)\n",
    "                targets = batch['polarity'].to(self.opt.device)\n",
    "                ##print('target:', targets)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                n_total += len(outputs)\n",
    "                loss_total += loss.item() * len(outputs)\n",
    "                if global_step % self.opt.log_step == 0:\n",
    "                    train_acc = n_correct / n_total\n",
    "                    train_loss = loss_total / n_total\n",
    "                    logger.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))\n",
    "\n",
    "            val_acc, val_f1 = self._evaluate_acc_f1(val_data_loader)\n",
    "            logger.info('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                max_val_epoch = i_epoch\n",
    "                if not os.path.exists('state_dict'):\n",
    "                    os.mkdir('state_dict')\n",
    "                #path = 'state_dict/{0}_{1}_val_acc_{2}'.format(self.opt.model_name, self.opt.dataset, round(val_acc, 4))\n",
    "                path = 'state_dict/model_file'\n",
    "                torch.save(self.model.state_dict(), path)\n",
    "                logger.info('>> saved: {}'.format(path))\n",
    "            if val_f1 > max_val_f1:\n",
    "                max_val_f1 = val_f1\n",
    "            if i_epoch - max_val_epoch >= self.opt.patience:\n",
    "                print('>> early stop.')\n",
    "                break\n",
    "\n",
    "        return path\n",
    "\n",
    "    def _evaluate_acc_f1(self, data_loader):\n",
    "        n_correct, n_total = 0, 0\n",
    "        t_targets_all, t_outputs_all = None, None\n",
    "        # switch model to evaluation mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i_batch, t_batch in enumerate(data_loader):\n",
    "                t_inputs = [t_batch[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                t_targets = t_batch['polarity'].to(self.opt.device)\n",
    "                \n",
    "                t_outputs = self.model(t_inputs)\n",
    "\n",
    "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "                n_total += len(t_outputs)\n",
    "\n",
    "                if t_targets_all is None:\n",
    "                    t_targets_all = t_targets\n",
    "                    t_outputs_all = t_outputs\n",
    "                else:\n",
    "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "        acc = n_correct / n_total\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='weighted')\n",
    "        \n",
    "        print(torch.argmax(t_outputs_all, -1).cpu())\n",
    "        return acc, f1\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        optimizer = self.opt.optimizer(_params, lr=self.opt.lr, weight_decay=self.opt.l2reg)\n",
    "\n",
    "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "        \n",
    "        print(test_data_loader)\n",
    "\n",
    "        self._reset_params()\n",
    "        best_model_path = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "        test_acc, test_f1 = self._evaluate_acc_f1(test_data_loader)\n",
    "        logger.info('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "XCeWm7vu78E6"
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', default='lcf_bert', type=str)\n",
    "    parser.add_argument('--dataset', default='restaurant', type=str, help='twitter, restaurant, laptop')\n",
    "    parser.add_argument('--optimizer', default='adam', type=str)\n",
    "    parser.add_argument('--initializer', default='xavier_uniform_', type=str)\n",
    "    parser.add_argument('--lr', default=5e-5, type=float, help='try 5e-5, 2e-5 for BERT, 1e-3 for others')\n",
    "    parser.add_argument('--dropout', default=0, type=float)\n",
    "    parser.add_argument('--l2reg', default=1e-4, type=float)\n",
    "    parser.add_argument('--num_epoch', default=2, type=int, help='try larger number for non-BERT models')\n",
    "    parser.add_argument('--batch_size', default=16, type=int, help='try 16, 32, 64 for BERT models')\n",
    "    parser.add_argument('--log_step', default=10, type=int)\n",
    "    parser.add_argument('--embed_dim', default=768, type=int)\n",
    "    parser.add_argument('--hidden_dim', default=768, type=int)\n",
    "    parser.add_argument('--bert_dim', default=768, type=int)\n",
    "    parser.add_argument('--pretrained_bert_name', default='bert-base-uncased', type=str)\n",
    "    parser.add_argument('--max_seq_len', default=200, type=int)\n",
    "    parser.add_argument('--polarities_dim', default=3, type=int)\n",
    "    parser.add_argument('--hops', default=3, type=int)\n",
    "    parser.add_argument('--patience', default=5, type=int)\n",
    "    parser.add_argument('--device', default='cuda:0', type=str)\n",
    "    parser.add_argument('--seed', default=1234, type=int, help='set seed for reproducibility')\n",
    "    parser.add_argument('--valset_ratio', default=0.29, type=float, help='set ratio between 0 and 1 for validation support')\n",
    "    # The following parameters are only valid for the lcf-bert model\n",
    "    parser.add_argument('--local_context_focus', default='cdw', type=str, help='local context focus mode, cdw or cdm')\n",
    "    parser.add_argument('--SRD', default=3, type=int, help='semantic-relative-distance, see the paper of LCF-BERT model')\n",
    "    opt = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "-0VK_r4N78E6"
   },
   "outputs": [],
   "source": [
    "    if opt.seed is not None:\n",
    "        random.seed(opt.seed)\n",
    "        numpy.random.seed(opt.seed)\n",
    "        torch.manual_seed(opt.seed)\n",
    "        torch.cuda.manual_seed(opt.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        os.environ['PYTHONHASHSEED'] = str(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "WvyKVoHI78E7",
    "outputId": "87be3e9d-7d33-4ea2-adf1-92ad82ef987d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 4.00 GiB total capacity; 3.38 GiB already allocated; 0 bytes free; 3.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11772\\2463392232.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInstructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[0mins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11772\\1999930810.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, opt)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer4Bert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_bert_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mbert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_bert_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             tokenizer = build_tokenizer(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    662\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 987\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 4.00 GiB total capacity; 3.38 GiB already allocated; 0 bytes free; 3.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "    model_classes = {\n",
    "        'lstm': LSTM,\n",
    "        'td_lstm': TD_LSTM,\n",
    "        'tc_lstm': TC_LSTM,\n",
    "        'atae_lstm': ATAE_LSTM,\n",
    "        'ian': IAN,\n",
    "        'memnet': MemNet,\n",
    "        'ram': RAM,\n",
    "        'cabasc': Cabasc,\n",
    "        'tnet_lf': TNet_LF,\n",
    "        'aoa': AOA,\n",
    "        'mgan': MGAN,\n",
    "        'asgcn': ASGCN,\n",
    "        'bert_spc': BERT_SPC,\n",
    "        'aen_bert': AEN_BERT,\n",
    "        'lcf_bert': LCF_BERT,\n",
    "    }\n",
    "    dataset_files = {\n",
    "        'twitter': {\n",
    "            'train': './datasets/acl-14-short-data/train.raw',\n",
    "            'test': './datasets/acl-14-short-data/test.raw'\n",
    "        },\n",
    "        'restaurant': {\n",
    "            'train': './datasets/semeval14/traindata2.xml.seg',\n",
    "            'test': './datasets/semeval14/testdata2.xml.seg'\n",
    "        },\n",
    "        'laptop': {\n",
    "            'train': './datasets/semeval14/Laptops_Train.xml.seg',\n",
    "            'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'\n",
    "        }\n",
    "    }\n",
    "    input_colses = {\n",
    "        'lstm': ['text_indices'],\n",
    "        'td_lstm': ['left_with_aspect_indices', 'right_with_aspect_indices'],\n",
    "        'tc_lstm': ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices'],\n",
    "        'atae_lstm': ['text_indices', 'aspect_indices'],\n",
    "        'ian': ['text_indices', 'aspect_indices'],\n",
    "        'memnet': ['context_indices', 'aspect_indices'],\n",
    "        'ram': ['text_indices', 'aspect_indices', 'left_indices'],\n",
    "        'cabasc': ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices'],\n",
    "        'tnet_lf': ['text_indices', 'aspect_indices', 'aspect_boundary'],\n",
    "        'aoa': ['text_indices', 'aspect_indices'],\n",
    "        'mgan': ['text_indices', 'aspect_indices', 'left_indices'],\n",
    "        'asgcn': ['text_indices', 'aspect_indices', 'left_indices', 'dependency_graph'],\n",
    "        'bert_spc': ['concat_bert_indices', 'concat_segments_indices'],\n",
    "        'aen_bert': ['text_bert_indices', 'aspect_bert_indices'],\n",
    "        'lcf_bert': ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices'],\n",
    "    }\n",
    "    initializers = {\n",
    "        'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
    "        'xavier_normal_': torch.nn.init.xavier_normal_,\n",
    "        'orthogonal_': torch.nn.init.orthogonal_,\n",
    "    }\n",
    "    optimizers = {\n",
    "        'adadelta': torch.optim.Adadelta,  # default lr=1.0\n",
    "        'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "        'adam': torch.optim.Adam,  # default lr=0.001\n",
    "        'adamax': torch.optim.Adamax,  # default lr=0.002\n",
    "        'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "        'rmsprop': torch.optim.RMSprop,  # default lr=0.01\n",
    "        'sgd': torch.optim.SGD,\n",
    "    }\n",
    "    opt.model_class = model_classes[opt.model_name]\n",
    "    opt.dataset_file = dataset_files[opt.dataset]\n",
    "    opt.inputs_cols = input_colses[opt.model_name]\n",
    "    opt.initializer = initializers[opt.initializer]\n",
    "    opt.optimizer = optimizers[opt.optimizer]\n",
    "    opt.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \\\n",
    "        if opt.device is None else torch.device(opt.device)\n",
    "\n",
    "    ins = Instructor(opt)\n",
    "    ins.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-EUofHW78E7",
    "outputId": "e19f0887-7c36-43f4-ad63-14c4ab4afa8c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To calculate F1-Score, Recall and Precision of all sentiment polarities of test dataset\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "######\n",
    "#From the prediction above for test dataset, results are entered into 'race_test2.csv' and imported to calculate all scores\n",
    "######\n",
    "input = pd.read_csv('datasets/race_test2.csv')\n",
    "print(\"F1 Score\")\n",
    "print('Weighted')\n",
    "print(f1_score(input['polarity'], input['predicted_polarity'], average='weighted'))\n",
    "print('Individual')\n",
    "print(f1_score(input['polarity'], input['predicted_polarity'], average=None))\n",
    "\n",
    "print(\"\\nRecall Score\")\n",
    "print('Weighted')\n",
    "print(recall_score(input['polarity'], input['predicted_polarity'], average='weighted'))\n",
    "print('Individual')\n",
    "print(recall_score(input['polarity'], input['predicted_polarity'], average=None))\n",
    "\n",
    "print(\"\\nPrecision Score\")\n",
    "print('Weighted')\n",
    "print(precision_score(input['polarity'], input['predicted_polarity'], average='weighted'))\n",
    "print('Individual')\n",
    "print(precision_score(input['polarity'], input['predicted_polarity'], average=None))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
